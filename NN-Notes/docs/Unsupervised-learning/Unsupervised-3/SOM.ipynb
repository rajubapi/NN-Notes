{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Organizing Map(SOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ***self-organizing map*** (SOM) or ***self-organizing feature map*** (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply **competitive learning** as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/SOM.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Self-Organizing Map (SOM) is a way to represent higher dimensional data in an usually 2-D or 3-D manner, such that similar data is grouped together.\n",
    "\n",
    "* It runs unsupervised and performs the grouping on its own.\n",
    "\n",
    "* Once the SOM converges, it can only classify new data.  It is unlike traditional neural nets which are continuously learning and adapting.\n",
    "\n",
    "* SOMs run in two phases:\n",
    "    1. *Training phase*: map is built, network organizes using a competitive process, it is trained using large numbers of inputs (or the same input vectors can be administered multiple times).\n",
    "    2. *Mapping phase*: new vectors are quickly given a location on the converged map, easily classifying or categorizing the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "So how does the SOM grid learn the shape of our data? Well, this is done in an iterative process, which is summarized in the following steps, and visualized in the animated GIF below:\n",
    "\n",
    "**Step 0**: Randomly position the grid’s neurons in the data space.\n",
    "\n",
    "**Step 1**: Select one data point, either randomly or systematically cycling through the dataset in order\n",
    "\n",
    "**Step 2**: Find the neuron that is closest to the chosen data point. This neuron is called the Best Matching Unit (BMU).\n",
    "\n",
    "**Step 3**: Move the BMU closer to that data point. The distance moved by the BMU is determined by a learning rate, which decreases after each iteration.\n",
    "\n",
    "**Step 4**: Move the BMU’s neighbors closer to that data point as well, with farther away neighbors moving less. Neighbors are identified using a radius around the BMU, and the value for this radius decreases after each iteration.\n",
    "\n",
    "**Step 5**: Update the learning rate and BMU radius, before repeating Steps 1 to 4. Iterate these steps until positions of neurons have been stabilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/som-explaingif.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visializing the SOM training\n",
    "\n",
    "The code below creats 200 random points and has a som trainig on it. \n",
    "\n",
    "In the gif below you can see how SOM learns the shape of the data and stabilises after some epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from som import SOM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499;    Neuron [3, 5];    \tSigma: 0.2941;    alpha: 0.0353\n",
      "Writing the gif file...\n",
      "Done\n",
      "Epoch 499;    Neuron [4, 3];    \tSigma: 0.2941;    alpha: 0.0353\n",
      "Writing the gif file...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "kohen_map = SOM(10, 10)\n",
    "\n",
    "# Create data\n",
    "data = np.random.randint(low=1, high=50, size=(200, 2))\n",
    "# Train\n",
    "kohen_map.fit(data, epochs=500)\n",
    "\n",
    "## For two classes\n",
    "data1 = np.random.randint(low=10, high=40, size=(100,2))\n",
    "data2 = np.random.randint(low=50, high=90, size=(100,2))\n",
    "som = SOM(10, 10)\n",
    "som.fit(data1, 500, data2=data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re-run this cell after executing the above cell to get the updated plot**\n",
    "\n",
    "<img src=\"./Images/som-training.gif\" />\n",
    "\n",
    "### Similarly, for two classes...\n",
    "\n",
    "<img src=\"./Images/som-training-two-classes.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
